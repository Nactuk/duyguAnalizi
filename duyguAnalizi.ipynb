{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0bd79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from alive_progress import alive_bar\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b403f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#veri ön işleme\n",
    "def preprocessing(series, remove_hashtag=False, remove_mentions=False, remove_links=False, remove_numbers=False,\n",
    "                  remove_short_text=False, remove_stopwords=False, lowercase=False, remove_punctuation=False,\n",
    "                  remove_rare_words=False, rare_limit=5):\n",
    "    if lowercase:\n",
    "        print(\"Lowercasing...\")\n",
    "        start = timer()\n",
    "        series = series.str.lower()\n",
    "        print(f\"Lowercasing is done in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_hashtag:\n",
    "        print(\"Removing hashtags...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'((#)[^\\s]*)\\b', '', regex=True)\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_mentions:\n",
    "        print(\"Removing mentions...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'((@)[^\\s]*)\\b', '', regex=True)\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_links:\n",
    "        print(\"Removing links...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'\\n', '', regex=True)\n",
    "        series = series.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_numbers:\n",
    "        print(\"Removing numbers...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'\\d+', '', regex=True)\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_short_text:\n",
    "        print(\"Removing short texts...\")\n",
    "        start = timer()\n",
    "        series = series.apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_stopwords:\n",
    "        print(\"Removing stopwords...\")\n",
    "        start = timer()\n",
    "        with open('assets/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "            stop = [line.strip() for line in f]\n",
    "        series = series.apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop]))\n",
    "        print(f\"Stopwords are removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_punctuation:\n",
    "        print(\"Removing punctuation...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r\"((')[^\\s]*)\\b\", '', regex=True)\n",
    "        series = series.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_rare_words:\n",
    "        print(\"Removing rare words...\")\n",
    "        start = timer()\n",
    "        whole_count = pd.Series(\" \".join(series).split()).value_counts()\n",
    "        print(f\"There are {whole_count.count()} words in the series\")\n",
    "        print(f\"%{round(whole_count[whole_count <= rare_limit].count() / whole_count.count() * 100, 2)} of \"\n",
    "              f\"words appear lesser than rare limit ({rare_limit}) times\")\n",
    "        to_remove = whole_count[whole_count <= rare_limit]\n",
    "        print(f\"Removing rare words...\")\n",
    "        series = series.apply(lambda x: \" \".join(x for x in x.split() if x not in to_remove))\n",
    "        print(f\"{len(to_remove)} rare words removed\")\n",
    "        print(f\"Removed in {timedelta(seconds=timer() - start)}\")\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48552f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duygu analizi fonksiyonu tanımlanıyor. Parametre olarak bir seri ve bir model adı alıyor. Model adı varsayılan olarak \"savasy/bert-base-turkish-sentiment-cased\" olarak belirleniyor.\n",
    "def sentiment(series, model_name=\"savasy/bert-base-turkish-sentiment-cased\"):\n",
    "    # transformers kütüphanesinden gerekli modüller import ediliyor.\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "    # label ve score adında iki boş liste oluşturuluyor. Bunlar sonuçları saklamak için kullanılacak.\n",
    "    label, score = [], []\n",
    "    # Model adına göre bir sınıflandırma modeli yükleniyor.\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    # Model adına göre bir tokenizer yükleniyor. Tokenizer, metni modelin anlayabileceği şekilde parçalara ayırır.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # pipeline fonksiyonu ile bir duygu analizi aracı oluşturuluyor. Bu araç, tokenizer ve modeli kullanarak metinleri duygu sınıflarına atar.\n",
    "    sa = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "    # Serinin içindeki her bir metin için bir döngü başlatılıyor. i, metnin indeksi, k ise metnin kendisi oluyor.\n",
    "    for i, k in enumerate(series):\n",
    "        # sa aracı ile metnin duygu analizi sonucu alınıyor. Sonuç, bir liste içinde bir sözlük olarak dönüyor. Sözlükte 'label' ve 'score' adında iki anahtar var. 'label', metnin duygu sınıfını, 'score' ise modelin bu sınıfa ne kadar güvendiğini gösteriyor.\n",
    "        result = sa(k)\n",
    "        # Sonuçtaki 'label' değerini label listesine ekliyoruz.\n",
    "        label.append(result[0]['label'])\n",
    "        # Sonuçtaki 'score' değerini score listesine ekliyoruz.\n",
    "        score.append(result[0]['score'])\n",
    "        # Eğer i sayısı 100'ün katı ise, yani 100, 200, 300 gibi ise, ekrana kaç metnin duygu analizinin yapıldığını yazdırıyoruz. Bu, işlemin ilerleyişini takip etmek için kullanılıyor.\n",
    "        if i % 100 == 0 and i != 0:\n",
    "            print(f\"Sentiment analysis of {i} tweets is done\")\n",
    "    # label ve score listelerini bir veri çerçevesine dönüştürüyoruz ve sütun isimlerini 'label' ve 'score' olarak atıyoruz.\n",
    "    return pd.DataFrame({'label': label, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a75069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(series):\n",
    "\"\"\"\n",
    "    Görevler \n",
    "    -----\n",
    "        Verilen serinin tfidf matrisini oluşturur.\n",
    "\n",
    "    Parametreler\n",
    "    ----------\n",
    "        series: pandas.Series\n",
    "        Dönüştürülecek seri.\n",
    "\n",
    "    Döndürür\n",
    "    -------\n",
    "        df_tfidf_vect: pandas.DataFrame\n",
    "            Tfidf matrisi.\n",
    "\"\"\"\n",
    "\n",
    "# TfidfVectorizer sınıfından bir nesne oluşturur. analyzer parametresi, kelime seviyesinde vektörleştirme yapacağını belirtir.\n",
    "tfidf_vectorized = TfidfVectorizer(analyzer='word')\n",
    "# Seriyi tfidf ağırlıklı bir matrise dönüştürür.\n",
    "tfidf_wm = tfidf_vectorized.fit_transform(series)\n",
    "# Matristeki sütun isimlerini, yani terimleri alır.\n",
    "tfidf_tokens = tfidf_vectorized.get_feature_names_out()\n",
    "# Matrisi bir veri çerçevesine dönüştürür ve sütun isimlerini terimler olarak atar.\n",
    "df_tfidf_vect = pd.DataFrame(data=tfidf_wm.toarray(), columns=tfidf_tokens)\n",
    "\n",
    "# Veri çerçevesini döndürür.\n",
    "return df_tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9a8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def get_models(x, y, test_size=0.25, random_state=15, classification=False, average='binary', order_type='acc', info=False):\n",
    "    \"\"\"\n",
    "        get_models adında bir fonksiyon tanımlanıyor. Bu fonksiyon, verilen x ve y değerleri için çeşitli makine öğrenmesi modellerini eğitir ve değerlendirir. Ayrıca test_size, random_state, classification, average, order_type ve info gibi parametreler alır. Bu parametrelerin anlamları şöyledir:\n",
    "        test_size: Verinin ne kadarının test kümesi olarak ayrılacağını belirtir. 0 ile 1 arasında bir değer alır. Varsayılan değer 0.25'tir.\n",
    "        random_state: Veriyi bölmeden önce karıştırmak için kullanılan rastgele sayı üretecinin tohumudur. Bir tam sayı veya None değeri alır. Varsayılan değer 15'tir.\n",
    "        classification: Eğitilecek modellerin sınıflandırma mı yoksa regresyon mu olduğunu belirtir. Bir mantıksal değer alır. Varsayılan değer False'tur.\n",
    "        average: Sınıflandırma modelleri için kullanılan metriklerin birden fazla sınıf için nasıl hesaplanacağını belirtir. 'binary', 'micro', 'macro', 'weighted' veya None değerlerinden birini alır. Varsayılan değer 'binary'dir.\n",
    "        order_type: Eğitilen modellerin hangi metriğe göre sıralanacağını belirtir. 'acc', 'precision', 'recall', 'f1' veya 'time' değerlerinden birini alır. Varsayılan değer 'acc'dir.\n",
    "        info: Eğitim sürecinde her model için bilgi mesajı yazdırılıp yazdırılmayacağını belirtir. Bir mantıksal değer alır. Varsayılan değer False'tur.\n",
    "\n",
    "    \"\"\"\n",
    "    # Veriyi test_size oranında eğitim ve test kümelerine ayırır. random_state parametresi ile karıştırma işlemini kontrol eder.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
    "    # Eğitilen modellerin sonuçlarını saklamak için boş bir liste oluşturur.\n",
    "    all_models = []\n",
    "    # order_type parametresinin karşılık geldiği metriklerin indekslerini bir sözlükte tutar.\n",
    "    order_types = {'acc': 1, 'precision': 2, 'recall': 3, 'f1': 4, 'time': 5}\n",
    "    \n",
    "    # classification parametresine göre eğitilecek modellerin listesini belirler. Bu liste, modelin adı ve nesnesi olarak ikililer içerir. Model nesneleri sklearn kütüphanesinden alınır. Bu örnekte, sınıflandırma için lojistik regresyon, k-en yakın komşu, karar ağacı, rastgele orman ve destek vektör makinesi modelleri; regresyon için ise doğrusal regresyon, ridge regresyon, lasso regresyon, elastik net regresyon ve destek vektör regresyonu modelleri kullanılmıştır. Ancak bu modeller değiştirilebilir.\n",
    "    if classification:\n",
    "        models = [\n",
    "            ('LR', LogisticRegression(random_state=random_state)),\n",
    "            ('KNN', KNeighborsClassifier()),\n",
    "            ('CART', DecisionTreeClassifier(random_state=random_state)),\n",
    "            ('RF', RandomForestClassifier(random_state=random_state)),\n",
    "            ('SVM', SVC(gamma='auto', random_state=random_state)),\n",
    "            ('GradientBoosting', GradientBoostingClassifier(random_state=random_state)),\n",
    "            (\"LightGBM\", LGBMClassifier(random_state=random_state)),\n",
    "            (\"CatBoost\", CatBoostClassifier(verbose=False, random_state=random_state))\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            ('LR', LinearRegression()),\n",
    "            (\"Ridge\", Ridge()),\n",
    "            (\"Lasso\", Lasso()),\n",
    "            (\"ElasticNet\", ElasticNet()),\n",
    "            ('KNN', KNeighborsRegressor()),\n",
    "            ('CART', DecisionTreeRegressor()),\n",
    "            ('RF', RandomForestRegressor()),\n",
    "            ('SVR', SVR()),\n",
    "            ('GBM', GradientBoostingRegressor()),\n",
    "            (\"XGBoost\", XGBRegressor()),\n",
    "            (\"LightGBM\", LGBMRegressor()),\n",
    "            (\"CatBoost\", CatBoostRegressor(verbose=False))\n",
    "        ]\n",
    "    \n",
    "    # Her bir model için bir döngü başlatır. name, modelin adı, model ise modelin nesnesidir.\n",
    "    for name, model in models:\n",
    "        # Eğitim süresini ölçmek için bir zamanlayıcı başlatır.\n",
    "        start = timer()\n",
    "        # info parametresi True ise, modelin eğitilmeye başlandığını ekrana yazdırır.\n",
    "        if info:\n",
    "            print(f\"{name} is training\")\n",
    "        # Modeli eğitim verisi ile eğitir.\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # classification parametresine göre değerlendirme metriklerini belirler. Sınıflandırma için doğruluk, kesinlik, geri çağırma ve f1 skoru; regresyon için ise eğitim ve test verileri için kök ortalama kare hata (RMSE) değerleri hesaplanır. Bu metrikler sklearn.metrics kütüphanesinden alınır. Ayrıca modelin adı ve eğitim süresi de kaydedilir.\n",
    "        if classification:\n",
    "            y_pred = model.predict(x_test)\n",
    "            # Doğruluk skorunu hesaplar.\n",
    "            acc_test = accuracy_score(y_test, y_pred)\n",
    "            # Kesinlik skorunu hesaplar. average parametresi ile birden fazla sınıf için hesaplama yöntemi belirlenir.\n",
    "            precision = precision_score(y_test, y_pred, average=average)\n",
    "            # Geri çağırma skorunu hesaplar. average parametresi ile birden fazla sınıf için hesaplama yöntemi belirlenir.\n",
    "            recall = recall_score(y_test, y_pred, average=average)\n",
    "            # F1 skorunu hesaplar. average parametresi ile birden fazla sınıf için hesaplama yöntemi belirlenir.\n",
    "            f1 = f1_score(y_test, y_pred, average=average)\n",
    "            # Modelin adı, doğruluk, kesinlik, geri çağırma, f1 skoru ve eğitim süresi gibi değerleri bir sözlükte tutar.\n",
    "            values = dict(name=name, acc_test=acc_test, precision=precision, recall=recall, f1=f1,\n",
    "                          train_time=str(timedelta(seconds=(timer() - start)))[-15:])\n",
    "        else:\n",
    "            # Modeli hem eğitim hem de test verileri ile tahmin eder.\n",
    "            y_pred_test = model.predict(x_test)\n",
    "            y_pred_train = model.predict(x_train)\n",
    "            # Eğitim ve test verileri için RMSE değerlerini hesaplar.\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            # Modelin adı, eğitim ve test RMSE değerleri gibi değerleri bir sözlükte tutar.\n",
    "            values = dict(name=name, RMSE_TRAIN=rmse_train, RMSE_TEST=rmse_test)\n",
    "        \n",
    "        # info parametresi True ise, modelin eğitim süresini ekrana yazdırır.\n",
    "        if info:\n",
    "            print(f\"{name} is done in {timedelta(seconds=(timer() - start))}\")\n",
    "        # Sözlüğü all_models listesine ekler.\n",
    "        all_models.append(values)\n",
    "\n",
    "    # all_models listesini bir veri çerçevesine dönüştürür ve order_type parametresine göre sıralar. order_type parametresi 'time' ise küçükten büyüğe, diğer durumlarda büyükten küçüğe sıralar.\n",
    "    all_models_df = pd.DataFrame(all_models).sort_values(by=list(all_models[0].keys())[order_types[order_type]],\n",
    "                                                          ascending=order_type != 'time')\n",
    "    # Tüm modellerin eğitildiğini ve order_type parametresine göre sıralandığını ekrana yazdırır.\n",
    "    print(f\"\\nAll models are done --- ordered by {order_type}\")\n",
    "    # Veri çerçevesini markdown formatında ekrana yazdırır.\n",
    "    print(all_models_df.to_markdown())\n",
    "    # Fonksiyon None değerini döndürür.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b683ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#türkçe karakterleri ingilizce yapan fonksiyon\n",
    "def tr_en_char_translate(series):\n",
    "    series = series.str.replace('ı', 'i')\n",
    "    series = series.str.replace('ü', 'u')\n",
    "    series = series.str.replace('ö', 'o')\n",
    "    series = series.str.replace('ğ', 'g')\n",
    "    series = series.str.replace('ş', 's')\n",
    "    series = series.str.replace('ç', 'c')\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130cb336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaş       150\n",
      "para      150\n",
      "teklif    150\n",
      "Name: topic, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>tweet</th>\n",
       "      <th>para</th>\n",
       "      <th>yaş</th>\n",
       "      <th>teklif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>para</td>\n",
       "      <td>Huzur, evlilik, para  gordum galiba sugar d4dd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>para</td>\n",
       "      <td>Evlilik için ilk yatırımımı (çeyrek altın)yapm...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>para</td>\n",
       "      <td>@Winb1r yüzük, zenginlik, evlilik???? zengin k...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>para</td>\n",
       "      <td>Evlilik pahalılaştıkça , zina ucuzlayacaktır .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>para</td>\n",
       "      <td>@lilililimarlen Çanım baa ölçüleri ve kriterle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic                                              tweet  para  yaş  teklif\n",
       "0  para  Huzur, evlilik, para  gordum galiba sugar d4dd...     1    0       0\n",
       "1  para  Evlilik için ilk yatırımımı (çeyrek altın)yapm...     1    0       0\n",
       "2  para  @Winb1r yüzük, zenginlik, evlilik???? zengin k...     1    0       0\n",
       "3  para     Evlilik pahalılaştıkça , zina ucuzlayacaktır .     1    0       0\n",
       "4  para  @lilililimarlen Çanım baa ölçüleri ve kriterle...     1    0       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning-set.csv adlı bir dosyayı okur ve df adlı bir veri çerçevesine atar. Dosyanın ayırıcı karakteri noktalı virgüldür.\n",
    "df = pd.read_csv(\"C:/Users/egeca/OneDrive/Masaüstü/İş Analtiği/FİNAL/learning-set.csv\", sep=';')\n",
    "\n",
    "# Veri çerçevesindeki topic adlı sütunun değerlerinin dağılımını ekrana yazdırır. topic, verilerin hangi konuya ait olduğunu gösteren bir sütundur.\n",
    "print(df.topic.value_counts())\n",
    "\n",
    "# Veri çerçevesine para adlı yeni bir sütun ekler. Bu sütun, topic sütununun değeri 'para' ise 1, değilse 0 olarak doldurulur. Böylece, verilerin para konulu olup olmadığını gösteren bir sütun oluşturulur.\n",
    "df['para'] = [1 if x == 'para' else 0 for x in df.topic]\n",
    "\n",
    "# Veri çerçevesine yaş adlı yeni bir sütun ekler. Bu sütun, topic sütununun değeri 'yaş' ise 1, değilse 0 olarak doldurulur. Böylece, verilerin yaş konulu olup olmadığını gösteren bir sütun oluşturulur.\n",
    "df['yaş'] = [1 if x == 'yaş' else 0 for x in df.topic]\n",
    "\n",
    "# Veri çerçevesine teklif adlı yeni bir sütun ekler. Bu sütun, topic sütununun değeri 'teklif' ise 1, değilse 0 olarak doldurulur. Böylece, verilerin teklif konulu olup olmadığını gösteren bir sütun oluşturulur.\n",
    "df['teklif'] = [1 if x == 'teklif' else 0 for x in df.topic]\n",
    "\n",
    "# Veri çerçevesinin ilk beş satırını ekrana yazdırır. Böylece, veri çerçevesinin yapısını ve içeriğini görebiliriz.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfe8fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing...\n",
      "Lowercasing is done in 0:00:00.001054\n",
      "Removing hashtags...\n",
      "Removed in 0:00:00.001130\n",
      "Removing mentions...\n",
      "Removed in 0:00:00.001142\n",
      "Removing links...\n",
      "Removed in 0:00:00.002162\n",
      "Removing numbers...\n",
      "Removed in 0:00:00.002020\n",
      "Removing punctuation...\n",
      "Removed in 0:00:00.002806\n",
      "Lowercasing...\n",
      "Lowercasing is done in 0:00:00.000942\n",
      "Removing hashtags...\n",
      "Removed in 0:00:00.001353\n",
      "Removing mentions...\n",
      "Removed in 0:00:00.001786\n",
      "Removing links...\n",
      "Removed in 0:00:00.001567\n",
      "Removing numbers...\n",
      "Removed in 0:00:00.001535\n",
      "Removing punctuation...\n",
      "Removed in 0:00:00.001849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>islenmis_veri</th>\n",
       "      <th>duygu_analizi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huzur, evlilik, para  gordum galiba sugar d4dd...</td>\n",
       "      <td>huzur evlilik para  gordum galiba sugar dddymi...</td>\n",
       "      <td>huzur evlilik para  gordum galiba sugar dddymi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evlilik için ilk yatırımımı (çeyrek altın)yapm...</td>\n",
       "      <td>evlilik için ilk yatırımımı çeyrek altınyapmış...</td>\n",
       "      <td>evlilik için ilk yatırımımı çeyrek altınyapmış...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Winb1r yüzük, zenginlik, evlilik???? zengin k...</td>\n",
       "      <td>yüzük zenginlik evlilik zengin koca buldum ga...</td>\n",
       "      <td>yüzük zenginlik evlilik zengin koca buldum ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evlilik pahalılaştıkça , zina ucuzlayacaktır .</td>\n",
       "      <td>evlilik pahalılaştıkça  zina ucuzlayacaktır</td>\n",
       "      <td>evlilik pahalılaştıkça  zina ucuzlayacaktır</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@lilililimarlen Çanım baa ölçüleri ve kriterle...</td>\n",
       "      <td>çanım baa ölçüleri ve kriterleri yaz ben çalı...</td>\n",
       "      <td>çanım baa ölçüleri ve kriterleri yaz ben çalı...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  Huzur, evlilik, para  gordum galiba sugar d4dd...   \n",
       "1  Evlilik için ilk yatırımımı (çeyrek altın)yapm...   \n",
       "2  @Winb1r yüzük, zenginlik, evlilik???? zengin k...   \n",
       "3     Evlilik pahalılaştıkça , zina ucuzlayacaktır .   \n",
       "4  @lilililimarlen Çanım baa ölçüleri ve kriterle...   \n",
       "\n",
       "                                       islenmis_veri  \\\n",
       "0  huzur evlilik para  gordum galiba sugar dddymi...   \n",
       "1  evlilik için ilk yatırımımı çeyrek altınyapmış...   \n",
       "2   yüzük zenginlik evlilik zengin koca buldum ga...   \n",
       "3       evlilik pahalılaştıkça  zina ucuzlayacaktır    \n",
       "4   çanım baa ölçüleri ve kriterleri yaz ben çalı...   \n",
       "\n",
       "                                       duygu_analizi  \n",
       "0  huzur evlilik para  gordum galiba sugar dddymi...  \n",
       "1  evlilik için ilk yatırımımı çeyrek altınyapmış...  \n",
       "2   yüzük zenginlik evlilik zengin koca buldum ga...  \n",
       "3       evlilik pahalılaştıkça  zina ucuzlayacaktır   \n",
       "4   çanım baa ölçüleri ve kriterleri yaz ben çalı...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veri önişleme\n",
    "df['islenmis_veri'] = preprocessing(df.tweet, remove_links=True, remove_mentions=True,\n",
    "                                           remove_hashtag=True, lowercase=True, remove_numbers=True,\n",
    "                                           remove_punctuation=True)\n",
    "\n",
    "df['duygu_analizi'] = preprocessing(df.tweet, remove_links=True, remove_mentions=True,\n",
    "                                           remove_hashtag=True, lowercase=True, remove_numbers=True,\n",
    "                                           remove_punctuation=True)\n",
    "\n",
    "df[['tweet', 'islenmis_veri', 'duygu_analizi']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2be094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>islenmis_veri</th>\n",
       "      <th>islenmis_veri_non_turkish_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huzur evlilik para  gordum galiba sugar dddymi...</td>\n",
       "      <td>huzur evlilik para  gordum galiba sugar dddymi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evlilik için ilk yatırımımı çeyrek altınyapmış...</td>\n",
       "      <td>evlilik icin ilk yatirimimi ceyrek altinyapmis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yüzük zenginlik evlilik zengin koca buldum ga...</td>\n",
       "      <td>yuzuk zenginlik evlilik zengin koca buldum ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evlilik pahalılaştıkça  zina ucuzlayacaktır</td>\n",
       "      <td>evlilik pahalilastikca  zina ucuzlayacaktir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>çanım baa ölçüleri ve kriterleri yaz ben çalı...</td>\n",
       "      <td>canim baa olculeri ve kriterleri yaz ben cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>evliliklan daha erken hayır</td>\n",
       "      <td>evliliklan daha erken hayir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>pedofil yok ama peygamberimiz evlilik yaşı i...</td>\n",
       "      <td>pedofil yok ama peygamberimiz evlilik yasi i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>türkiye afganistanı örnek alsınkızlar okula g...</td>\n",
       "      <td>turkiye afganistani ornek alsinkizlar okula g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>yaşında taş gibi karı ne var bunda  yaşındak...</td>\n",
       "      <td>yasinda tas gibi kari ne var bunda  yasindak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>evlilik yaşı falım  çıktı nolur gerçek olmasın...</td>\n",
       "      <td>evlilik yasi falim  cikti nolur gercek olmasin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         islenmis_veri  \\\n",
       "0    huzur evlilik para  gordum galiba sugar dddymi...   \n",
       "1    evlilik için ilk yatırımımı çeyrek altınyapmış...   \n",
       "2     yüzük zenginlik evlilik zengin koca buldum ga...   \n",
       "3         evlilik pahalılaştıkça  zina ucuzlayacaktır    \n",
       "4     çanım baa ölçüleri ve kriterleri yaz ben çalı...   \n",
       "..                                                 ...   \n",
       "445                        evliliklan daha erken hayır   \n",
       "446    pedofil yok ama peygamberimiz evlilik yaşı i...   \n",
       "447   türkiye afganistanı örnek alsınkızlar okula g...   \n",
       "448    yaşında taş gibi karı ne var bunda  yaşındak...   \n",
       "449  evlilik yaşı falım  çıktı nolur gerçek olmasın...   \n",
       "\n",
       "                        islenmis_veri_non_turkish_char  \n",
       "0    huzur evlilik para  gordum galiba sugar dddymi...  \n",
       "1    evlilik icin ilk yatirimimi ceyrek altinyapmis...  \n",
       "2     yuzuk zenginlik evlilik zengin koca buldum ga...  \n",
       "3         evlilik pahalilastikca  zina ucuzlayacaktir   \n",
       "4     canim baa olculeri ve kriterleri yaz ben cali...  \n",
       "..                                                 ...  \n",
       "445                        evliliklan daha erken hayir  \n",
       "446    pedofil yok ama peygamberimiz evlilik yasi i...  \n",
       "447   turkiye afganistani ornek alsinkizlar okula g...  \n",
       "448    yasinda tas gibi kari ne var bunda  yasindak...  \n",
       "449  evlilik yasi falim  cikti nolur gercek olmasin...  \n",
       "\n",
       "[450 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#türkçe karakter değişimi\n",
    "df['islenmis_veri_non_turkish_char'] = tr_en_char_translate(df['islenmis_veri'])\n",
    "#gösterimi\n",
    "df[['islenmis_veri', 'islenmis_veri_non_turkish_char']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af7cea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing rare words...\n",
      "There are 4117 words in the series\n",
      "%0.0 of words appear lesser than rare limit (0) times\n",
      "Removing rare words...\n",
      "0 rare words removed\n",
      "Removed in 0:00:00.012474\n"
     ]
    }
   ],
   "source": [
    "#nadir kelimelerin silinimi ve bunu deneme sütunu oluşturup ona eklemek\n",
    "df['deneme'] = preprocessing(df.islenmis_veri, remove_rare_words=True, rare_limit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ab5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_set = create_tfidf(df['islenmis_veri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e469a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR is training\n",
      "LR is done in 0:00:00.344677\n",
      "KNN is training\n",
      "KNN is done in 0:00:00.259508\n",
      "CART is training\n",
      "CART is done in 0:00:00.113156\n",
      "RF is training\n",
      "RF is done in 0:00:00.518367\n",
      "SVM is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeca\\Python_Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is done in 0:00:00.350977\n",
      "GradientBoosting is training\n",
      "GradientBoosting is done in 0:00:02.602159\n",
      "LightGBM is training\n",
      "[LightGBM] [Info] Number of positive: 108, number of negative: 229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 369\n",
      "[LightGBM] [Info] Number of data points in the train set: 337, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.320475 -> initscore=-0.751591\n",
      "[LightGBM] [Info] Start training from score -0.751591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM is done in 0:00:00.140474\n",
      "CatBoost is training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost is done in 0:00:11.361853\n",
      "\n",
      "All models are done --- ordered by acc\n",
      "|    | name             |   acc_test |   precision |    recall |       f1 | train_time     |\n",
      "|---:|:-----------------|-----------:|------------:|----------:|---------:|:---------------|\n",
      "|  4 | SVM              |   0.628319 |    0        | 0         | 0        | 0:00:00.350969 |\n",
      "|  0 | LR               |   0.663717 |    1        | 0.0952381 | 0.173913 | 0:00:00.344668 |\n",
      "|  6 | LightGBM         |   0.663717 |    0.555556 | 0.47619   | 0.512821 | 0:00:00.140466 |\n",
      "|  3 | RF               |   0.725664 |    1        | 0.261905  | 0.415094 | 0:00:00.518358 |\n",
      "|  7 | CatBoost         |   0.725664 |    0.789474 | 0.357143  | 0.491803 | 0:00:11.361845 |\n",
      "|  2 | CART             |   0.761062 |    0.727273 | 0.571429  | 0.64     | 0:00:00.113148 |\n",
      "|  5 | GradientBoosting |   0.778761 |    0.904762 | 0.452381  | 0.603175 | 0:00:02.602150 |\n",
      "|  1 | KNN              |   0.80531  |    0.857143 | 0.571429  | 0.685714 | 0:00:00.259500 |\n",
      "LR is training\n",
      "LR is done in 0:00:00.112240\n",
      "KNN is training\n",
      "KNN is done in 0:00:00.062107\n",
      "CART is training\n",
      "CART is done in 0:00:00.137155\n",
      "RF is training\n",
      "RF is done in 0:00:00.503707\n",
      "SVM is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeca\\Python_Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is done in 0:00:00.345699\n",
      "GradientBoosting is training\n",
      "GradientBoosting is done in 0:00:02.519223\n",
      "LightGBM is training\n",
      "[LightGBM] [Info] Number of positive: 118, number of negative: 219\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 369\n",
      "[LightGBM] [Info] Number of data points in the train set: 337, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350148 -> initscore=-0.618387\n",
      "[LightGBM] [Info] Start training from score -0.618387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM is done in 0:00:00.096720\n",
      "CatBoost is training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost is done in 0:00:11.033859\n",
      "\n",
      "All models are done --- ordered by acc\n",
      "|    | name             |   acc_test |   precision |   recall |       f1 | train_time     |\n",
      "|---:|:-----------------|-----------:|------------:|---------:|---------:|:---------------|\n",
      "|  4 | SVM              |   0.716814 |    0        |  0       | 0        | 0:00:00.345691 |\n",
      "|  0 | LR               |   0.761062 |    1        |  0.15625 | 0.27027  | 0:00:00.112231 |\n",
      "|  1 | KNN              |   0.858407 |    0.7      |  0.875   | 0.777778 | 0:00:00.062099 |\n",
      "|  2 | CART             |   0.858407 |    0.785714 |  0.6875  | 0.733333 | 0:00:00.137146 |\n",
      "|  3 | RF               |   0.884956 |    0.952381 |  0.625   | 0.754717 | 0:00:00.503698 |\n",
      "|  6 | LightGBM         |   0.884956 |    0.88     |  0.6875  | 0.77193  | 0:00:00.096712 |\n",
      "|  7 | CatBoost         |   0.893805 |    0.954545 |  0.65625 | 0.777778 | 0:00:11.033849 |\n",
      "|  5 | GradientBoosting |   0.902655 |    0.862069 |  0.78125 | 0.819672 | 0:00:02.519213 |\n",
      "LR is training\n",
      "LR is done in 0:00:00.103840\n",
      "KNN is training\n",
      "KNN is done in 0:00:00.061709\n",
      "CART is training\n",
      "CART is done in 0:00:00.135948\n",
      "RF is training\n",
      "RF is done in 0:00:00.477660\n",
      "SVM is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeca\\Python_Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is done in 0:00:00.306335\n",
      "GradientBoosting is training\n",
      "GradientBoosting is done in 0:00:02.468029\n",
      "LightGBM is training\n",
      "[LightGBM] [Info] Number of positive: 111, number of negative: 226\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 369\n",
      "[LightGBM] [Info] Number of data points in the train set: 337, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.329377 -> initscore=-0.711005\n",
      "[LightGBM] [Info] Start training from score -0.711005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM is done in 0:00:00.091540\n",
      "CatBoost is training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost is done in 0:00:11.046082\n",
      "\n",
      "All models are done --- ordered by acc\n",
      "|    | name             |   acc_test |   precision |   recall |       f1 | train_time     |\n",
      "|---:|:-----------------|-----------:|------------:|---------:|---------:|:---------------|\n",
      "|  4 | SVM              |   0.654867 |    0        | 0        | 0        | 0:00:00.306327 |\n",
      "|  0 | LR               |   0.707965 |    1        | 0.153846 | 0.266667 | 0:00:00.103829 |\n",
      "|  6 | LightGBM         |   0.761062 |    0.730769 | 0.487179 | 0.584615 | 0:00:00.091531 |\n",
      "|  3 | RF               |   0.787611 |    1        | 0.384615 | 0.555556 | 0:00:00.477651 |\n",
      "|  1 | KNN              |   0.814159 |    0.875    | 0.538462 | 0.666667 | 0:00:00.061700 |\n",
      "|  2 | CART             |   0.814159 |    0.8      | 0.615385 | 0.695652 | 0:00:00.135939 |\n",
      "|  5 | GradientBoosting |   0.814159 |    0.909091 | 0.512821 | 0.655738 | 0:00:02.468021 |\n",
      "|  7 | CatBoost         |   0.831858 |    0.954545 | 0.538462 | 0.688525 | 0:00:11.046074 |\n"
     ]
    }
   ],
   "source": [
    "#sırasıyla teste edip en başarılı modeli bulma\n",
    "get_models(tf_idf_set, df.para, classification=True, info=True)\n",
    "# para, en iyi KNN ile çalıştı.\n",
    "\n",
    "get_models(tf_idf_set, df.teklif, classification=True, info=True)\n",
    "# teklif, en iyi GradientBoosting ile çalıştı.\n",
    "\n",
    "get_models(tf_idf_set, df.yaş, classification=True, info=True)\n",
    "# yaş, en iyi CART ile çalıştı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864a2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing...\n",
      "Lowercasing is done in 0:00:00.047459\n",
      "Removing hashtags...\n",
      "Removed in 0:00:00.026917\n",
      "Removing mentions...\n",
      "Removed in 0:00:00.038884\n",
      "Removing links...\n",
      "Removed in 0:00:00.072261\n",
      "Removing numbers...\n",
      "Removed in 0:00:00.093250\n",
      "Removing punctuation...\n",
      "Removed in 0:00:00.121729\n",
      "Lowercasing...\n",
      "Lowercasing is done in 0:00:00.041842\n",
      "Removing hashtags...\n",
      "Removed in 0:00:00.028282\n",
      "Removing mentions...\n",
      "Removed in 0:00:00.037766\n",
      "Removing links...\n",
      "Removed in 0:00:00.069508\n",
      "Removing numbers...\n",
      "Removed in 0:00:00.092101\n",
      "Removing punctuation...\n",
      "Removed in 0:00:00.122769\n"
     ]
    }
   ],
   "source": [
    "#30.000 satırlık evlilik_1 veri setinin okunulması ve veri ön işlemesi\n",
    "evlilik = pd.read_csv(\"C:/Users/egeca/OneDrive/Masaüstü/İş Analtiği/FİNAL/evlilik_1.csv\", sep=',')\n",
    "\n",
    "evlilik['temizlenmiş'] = preprocessing(evlilik.tweet, remove_links=True, remove_mentions=True,\n",
    "                                           remove_hashtag=True, lowercase=True, remove_numbers=True,\n",
    "                                           remove_punctuation=True)\n",
    "\n",
    "evlilik['duygu_analizi'] = preprocessing(evlilik.tweet, remove_links=True, remove_mentions=True,\n",
    "                                           remove_hashtag=True, lowercase=True, remove_numbers=True,\n",
    "                                           remove_punctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adac64a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitti\n"
     ]
    }
   ],
   "source": [
    "# Makine Öğrenmesi\n",
    "tfidf_vectorized = TfidfVectorizer(analyzer='word')\n",
    "tf_idf_donusturucu = tfidf_vectorized.fit(df['islenmis_veri'])\n",
    "ogrenme_seti = tf_idf_donusturucu.transform(df['islenmis_veri'])\n",
    "ogrenme_seti_vectoru = pd.DataFrame(data=ogrenme_seti.toarray(), columns=tf_idf_donusturucu.get_feature_names_out())\n",
    "tahmin_seti = tf_idf_donusturucu.transform(evlilik.temizlenmiş)\n",
    "tahmin_seti_vectoru = pd.DataFrame(data=tahmin_seti.toarray(), columns=tf_idf_donusturucu.get_feature_names_out())\n",
    "print(\"bitti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93255d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>temizlenmiş</th>\n",
       "      <th>duygu_analizi</th>\n",
       "      <th>para</th>\n",
       "      <th>teklif</th>\n",
       "      <th>yaş</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Üniversitemizin güllerinden Ekrem kardeşimizi ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üniversitemizin güllerinden ekrem kardeşimizi ...</td>\n",
       "      <td>üniversitemizin güllerinden ekrem kardeşimizi ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huzur, evlilik, para  gordum galiba sugar d4dd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>huzur evlilik para  gordum galiba sugar dddymi...</td>\n",
       "      <td>huzur evlilik para  gordum galiba sugar dddymi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tribünde önümüzde evlilik teklifi edildi. dald...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tribünde önümüzde evlilik teklifi edildi daldı...</td>\n",
       "      <td>tribünde önümüzde evlilik teklifi edildi daldı...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Komucuuu Nasıl önleyebiliriz ? sanırım evlili...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nasıl önleyebiliriz  sanırım evlilik öncesi b...</td>\n",
       "      <td>nasıl önleyebiliriz  sanırım evlilik öncesi b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Winb1r Yüzük nişan evlilik bu kadar denk gelm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yüzük nişan evlilik bu kadar denk gelmez evre...</td>\n",
       "      <td>yüzük nişan evlilik bu kadar denk gelmez evre...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  Unnamed: 1  \\\n",
       "0  Üniversitemizin güllerinden Ekrem kardeşimizi ...         NaN   \n",
       "1  Huzur, evlilik, para  gordum galiba sugar d4dd...         NaN   \n",
       "2  tribünde önümüzde evlilik teklifi edildi. dald...         NaN   \n",
       "3  @Komucuuu Nasıl önleyebiliriz ? sanırım evlili...         NaN   \n",
       "4  @Winb1r Yüzük nişan evlilik bu kadar denk gelm...         NaN   \n",
       "\n",
       "                                         temizlenmiş  \\\n",
       "0  üniversitemizin güllerinden ekrem kardeşimizi ...   \n",
       "1  huzur evlilik para  gordum galiba sugar dddymi...   \n",
       "2  tribünde önümüzde evlilik teklifi edildi daldı...   \n",
       "3   nasıl önleyebiliriz  sanırım evlilik öncesi b...   \n",
       "4   yüzük nişan evlilik bu kadar denk gelmez evre...   \n",
       "\n",
       "                                       duygu_analizi  para  teklif  yaş  \n",
       "0  üniversitemizin güllerinden ekrem kardeşimizi ...     0       0    1  \n",
       "1  huzur evlilik para  gordum galiba sugar dddymi...     1       0    0  \n",
       "2  tribünde önümüzde evlilik teklifi edildi daldı...     0       1    0  \n",
       "3   nasıl önleyebiliriz  sanırım evlilik öncesi b...     1       0    0  \n",
       "4   yüzük nişan evlilik bu kadar denk gelmez evre...     0       1    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para sütununda KNN kullandık çünkü en başarılı oydu\n",
    "knn_model_para = KNeighborsClassifier()\n",
    "knn_model_para.fit(ogrenme_seti_vectoru, df.para)\n",
    "evlilik['para'] = knn_model_para.predict(tahmin_seti_vectoru)\n",
    "\n",
    "# Teklif  sütununda - GradientBoosting kullandık çünkü en başarılı oydu\n",
    "gb_model_teklif = GradientBoostingClassifier()\n",
    "gb_model_teklif.fit(ogrenme_seti_vectoru, df.teklif)\n",
    "evlilik['teklif'] = gb_model_teklif.predict(tahmin_seti_vectoru)\n",
    "\n",
    "# Yaş  sütununda - Decision Tree (CART) kullandık çünkü en başarılı oydu\n",
    "cart_model_yas = DecisionTreeClassifier()\n",
    "cart_model_yas.fit(ogrenme_seti_vectoru, df.yaş)\n",
    "evlilik['yaş'] = cart_model_yas.predict(tahmin_seti_vectoru)\n",
    "\n",
    "evlilik.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f1a7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14796"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# afet, cevre ve iklimin 0 olduğu satırlar\n",
    "len(evlilik[(evlilik.yaş == 0) & (evlilik.teklif == 0) & (evlilik.para == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24909318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis of 100 tweets is done\n",
      "Sentiment analysis of 200 tweets is done\n",
      "Sentiment analysis of 300 tweets is done\n",
      "Sentiment analysis of 400 tweets is done\n",
      "Sentiment analysis of 500 tweets is done\n",
      "Sentiment analysis of 600 tweets is done\n",
      "Sentiment analysis of 700 tweets is done\n",
      "Sentiment analysis of 800 tweets is done\n",
      "Sentiment analysis of 900 tweets is done\n",
      "Sentiment analysis of 1000 tweets is done\n",
      "Sentiment analysis of 1100 tweets is done\n",
      "Sentiment analysis of 1200 tweets is done\n",
      "Sentiment analysis of 1300 tweets is done\n",
      "Sentiment analysis of 1400 tweets is done\n",
      "Sentiment analysis of 1500 tweets is done\n",
      "Sentiment analysis of 1600 tweets is done\n",
      "Sentiment analysis of 1700 tweets is done\n",
      "Sentiment analysis of 1800 tweets is done\n",
      "Sentiment analysis of 1900 tweets is done\n",
      "Sentiment analysis of 2000 tweets is done\n",
      "Sentiment analysis of 2100 tweets is done\n",
      "Sentiment analysis of 2200 tweets is done\n",
      "Sentiment analysis of 2300 tweets is done\n",
      "Sentiment analysis of 2400 tweets is done\n",
      "Sentiment analysis of 2500 tweets is done\n",
      "Sentiment analysis of 2600 tweets is done\n",
      "Sentiment analysis of 2700 tweets is done\n",
      "Sentiment analysis of 2800 tweets is done\n",
      "Sentiment analysis of 2900 tweets is done\n",
      "Sentiment analysis of 3000 tweets is done\n",
      "Sentiment analysis of 3100 tweets is done\n",
      "Sentiment analysis of 3200 tweets is done\n",
      "Sentiment analysis of 3300 tweets is done\n",
      "Sentiment analysis of 3400 tweets is done\n",
      "Sentiment analysis of 3500 tweets is done\n",
      "Sentiment analysis of 3600 tweets is done\n",
      "Sentiment analysis of 3700 tweets is done\n",
      "Sentiment analysis of 3800 tweets is done\n",
      "Sentiment analysis of 3900 tweets is done\n",
      "Sentiment analysis of 4000 tweets is done\n",
      "Sentiment analysis of 4100 tweets is done\n",
      "Sentiment analysis of 4200 tweets is done\n",
      "Sentiment analysis of 4300 tweets is done\n",
      "Sentiment analysis of 4400 tweets is done\n",
      "Sentiment analysis of 4500 tweets is done\n",
      "Sentiment analysis of 4600 tweets is done\n",
      "Sentiment analysis of 4700 tweets is done\n",
      "Sentiment analysis of 4800 tweets is done\n",
      "Sentiment analysis of 4900 tweets is done\n",
      "Sentiment analysis of 5000 tweets is done\n",
      "Sentiment analysis of 5100 tweets is done\n",
      "Sentiment analysis of 5200 tweets is done\n",
      "Sentiment analysis of 5300 tweets is done\n",
      "Sentiment analysis of 5400 tweets is done\n",
      "Sentiment analysis of 5500 tweets is done\n",
      "Sentiment analysis of 5600 tweets is done\n",
      "Sentiment analysis of 5700 tweets is done\n",
      "Sentiment analysis of 5800 tweets is done\n",
      "Sentiment analysis of 5900 tweets is done\n",
      "Sentiment analysis of 6000 tweets is done\n",
      "Sentiment analysis of 6100 tweets is done\n",
      "Sentiment analysis of 6200 tweets is done\n",
      "Sentiment analysis of 6300 tweets is done\n",
      "Sentiment analysis of 6400 tweets is done\n",
      "Sentiment analysis of 6500 tweets is done\n",
      "Sentiment analysis of 6600 tweets is done\n",
      "Sentiment analysis of 6700 tweets is done\n",
      "Sentiment analysis of 6800 tweets is done\n",
      "Sentiment analysis of 6900 tweets is done\n",
      "Sentiment analysis of 7000 tweets is done\n",
      "Sentiment analysis of 7100 tweets is done\n",
      "Sentiment analysis of 7200 tweets is done\n",
      "Sentiment analysis of 7300 tweets is done\n",
      "Sentiment analysis of 7400 tweets is done\n",
      "Sentiment analysis of 7500 tweets is done\n",
      "Sentiment analysis of 7600 tweets is done\n",
      "Sentiment analysis of 7700 tweets is done\n",
      "Sentiment analysis of 7800 tweets is done\n",
      "Sentiment analysis of 7900 tweets is done\n",
      "Sentiment analysis of 8000 tweets is done\n",
      "Sentiment analysis of 8100 tweets is done\n",
      "Sentiment analysis of 8200 tweets is done\n",
      "Sentiment analysis of 8300 tweets is done\n",
      "Sentiment analysis of 8400 tweets is done\n",
      "Sentiment analysis of 8500 tweets is done\n",
      "Sentiment analysis of 8600 tweets is done\n",
      "Sentiment analysis of 8700 tweets is done\n",
      "Sentiment analysis of 8800 tweets is done\n",
      "Sentiment analysis of 8900 tweets is done\n",
      "Sentiment analysis of 9000 tweets is done\n",
      "Sentiment analysis of 9100 tweets is done\n",
      "Sentiment analysis of 9200 tweets is done\n",
      "Sentiment analysis of 9300 tweets is done\n",
      "Sentiment analysis of 9400 tweets is done\n",
      "Sentiment analysis of 9500 tweets is done\n",
      "Sentiment analysis of 9600 tweets is done\n",
      "Sentiment analysis of 9700 tweets is done\n",
      "Sentiment analysis of 9800 tweets is done\n",
      "Sentiment analysis of 9900 tweets is done\n",
      "Sentiment analysis of 10000 tweets is done\n",
      "Sentiment analysis of 10100 tweets is done\n",
      "Sentiment analysis of 10200 tweets is done\n",
      "Sentiment analysis of 10300 tweets is done\n",
      "Sentiment analysis of 10400 tweets is done\n",
      "Sentiment analysis of 10500 tweets is done\n",
      "Sentiment analysis of 10600 tweets is done\n",
      "Sentiment analysis of 10700 tweets is done\n",
      "Sentiment analysis of 10800 tweets is done\n",
      "Sentiment analysis of 10900 tweets is done\n",
      "Sentiment analysis of 11000 tweets is done\n",
      "Sentiment analysis of 11100 tweets is done\n",
      "Sentiment analysis of 11200 tweets is done\n",
      "Sentiment analysis of 11300 tweets is done\n",
      "Sentiment analysis of 11400 tweets is done\n",
      "Sentiment analysis of 11500 tweets is done\n",
      "Sentiment analysis of 11600 tweets is done\n",
      "Sentiment analysis of 11700 tweets is done\n",
      "Sentiment analysis of 11800 tweets is done\n",
      "Sentiment analysis of 11900 tweets is done\n",
      "Sentiment analysis of 12000 tweets is done\n",
      "Sentiment analysis of 12100 tweets is done\n",
      "Sentiment analysis of 12200 tweets is done\n",
      "Sentiment analysis of 12300 tweets is done\n",
      "Sentiment analysis of 12400 tweets is done\n",
      "Sentiment analysis of 12500 tweets is done\n",
      "Sentiment analysis of 12600 tweets is done\n",
      "Sentiment analysis of 12700 tweets is done\n",
      "Sentiment analysis of 12800 tweets is done\n",
      "Sentiment analysis of 12900 tweets is done\n",
      "Sentiment analysis of 13000 tweets is done\n",
      "Sentiment analysis of 13100 tweets is done\n",
      "Sentiment analysis of 13200 tweets is done\n",
      "Sentiment analysis of 13300 tweets is done\n",
      "Sentiment analysis of 13400 tweets is done\n",
      "Sentiment analysis of 13500 tweets is done\n",
      "Sentiment analysis of 13600 tweets is done\n",
      "Sentiment analysis of 13700 tweets is done\n",
      "Sentiment analysis of 13800 tweets is done\n",
      "Sentiment analysis of 13900 tweets is done\n",
      "Sentiment analysis of 14000 tweets is done\n",
      "Sentiment analysis of 14100 tweets is done\n",
      "Sentiment analysis of 14200 tweets is done\n",
      "Sentiment analysis of 14300 tweets is done\n",
      "Sentiment analysis of 14400 tweets is done\n",
      "Sentiment analysis of 14500 tweets is done\n",
      "Sentiment analysis of 14600 tweets is done\n",
      "Sentiment analysis of 14700 tweets is done\n",
      "Sentiment analysis of 14800 tweets is done\n",
      "Sentiment analysis of 14900 tweets is done\n",
      "Sentiment analysis of 15000 tweets is done\n",
      "Sentiment analysis of 15100 tweets is done\n",
      "Sentiment analysis of 15200 tweets is done\n",
      "Sentiment analysis of 15300 tweets is done\n",
      "Sentiment analysis of 15400 tweets is done\n",
      "Sentiment analysis of 15500 tweets is done\n",
      "Sentiment analysis of 15600 tweets is done\n",
      "Sentiment analysis of 15700 tweets is done\n",
      "Sentiment analysis of 15800 tweets is done\n",
      "Sentiment analysis of 15900 tweets is done\n",
      "Sentiment analysis of 16000 tweets is done\n",
      "Sentiment analysis of 16100 tweets is done\n",
      "Sentiment analysis of 16200 tweets is done\n",
      "Sentiment analysis of 16300 tweets is done\n",
      "Sentiment analysis of 16400 tweets is done\n",
      "Sentiment analysis of 16500 tweets is done\n",
      "Sentiment analysis of 16600 tweets is done\n",
      "Sentiment analysis of 16700 tweets is done\n",
      "Sentiment analysis of 16800 tweets is done\n",
      "Sentiment analysis of 16900 tweets is done\n",
      "Sentiment analysis of 17000 tweets is done\n",
      "Sentiment analysis of 17100 tweets is done\n",
      "Sentiment analysis of 17200 tweets is done\n",
      "Sentiment analysis of 17300 tweets is done\n",
      "Sentiment analysis of 17400 tweets is done\n",
      "Sentiment analysis of 17500 tweets is done\n",
      "Sentiment analysis of 17600 tweets is done\n",
      "Sentiment analysis of 17700 tweets is done\n",
      "Sentiment analysis of 17800 tweets is done\n",
      "Sentiment analysis of 17900 tweets is done\n",
      "Sentiment analysis of 18000 tweets is done\n",
      "Sentiment analysis of 18100 tweets is done\n",
      "Sentiment analysis of 18200 tweets is done\n",
      "Sentiment analysis of 18300 tweets is done\n",
      "Sentiment analysis of 18400 tweets is done\n",
      "Sentiment analysis of 18500 tweets is done\n",
      "Sentiment analysis of 18600 tweets is done\n",
      "Sentiment analysis of 18700 tweets is done\n",
      "Sentiment analysis of 18800 tweets is done\n",
      "Sentiment analysis of 18900 tweets is done\n",
      "Sentiment analysis of 19000 tweets is done\n",
      "Sentiment analysis of 19100 tweets is done\n",
      "Sentiment analysis of 19200 tweets is done\n",
      "Sentiment analysis of 19300 tweets is done\n",
      "Sentiment analysis of 19400 tweets is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis of 19500 tweets is done\n",
      "Sentiment analysis of 19600 tweets is done\n",
      "Sentiment analysis of 19700 tweets is done\n",
      "Sentiment analysis of 19800 tweets is done\n",
      "Sentiment analysis of 19900 tweets is done\n",
      "Sentiment analysis of 20000 tweets is done\n",
      "Sentiment analysis of 20100 tweets is done\n",
      "Sentiment analysis of 20200 tweets is done\n",
      "Sentiment analysis of 20300 tweets is done\n",
      "Sentiment analysis of 20400 tweets is done\n",
      "Sentiment analysis of 20500 tweets is done\n",
      "Sentiment analysis of 20600 tweets is done\n",
      "Sentiment analysis of 20700 tweets is done\n",
      "Sentiment analysis of 20800 tweets is done\n",
      "Sentiment analysis of 20900 tweets is done\n",
      "Sentiment analysis of 21000 tweets is done\n",
      "Sentiment analysis of 21100 tweets is done\n",
      "Sentiment analysis of 21200 tweets is done\n",
      "Sentiment analysis of 21300 tweets is done\n",
      "Sentiment analysis of 21400 tweets is done\n",
      "Sentiment analysis of 21500 tweets is done\n",
      "Sentiment analysis of 21600 tweets is done\n",
      "Sentiment analysis of 21700 tweets is done\n",
      "Sentiment analysis of 21800 tweets is done\n",
      "Sentiment analysis of 21900 tweets is done\n",
      "Sentiment analysis of 22000 tweets is done\n",
      "Sentiment analysis of 22100 tweets is done\n",
      "Sentiment analysis of 22200 tweets is done\n",
      "Sentiment analysis of 22300 tweets is done\n",
      "Sentiment analysis of 22400 tweets is done\n",
      "Sentiment analysis of 22500 tweets is done\n",
      "Sentiment analysis of 22600 tweets is done\n",
      "Sentiment analysis of 22700 tweets is done\n",
      "Sentiment analysis of 22800 tweets is done\n",
      "Sentiment analysis of 22900 tweets is done\n",
      "Sentiment analysis of 23000 tweets is done\n",
      "Sentiment analysis of 23100 tweets is done\n",
      "Sentiment analysis of 23200 tweets is done\n",
      "Sentiment analysis of 23300 tweets is done\n",
      "Sentiment analysis of 23400 tweets is done\n",
      "Sentiment analysis of 23500 tweets is done\n",
      "Sentiment analysis of 23600 tweets is done\n",
      "Sentiment analysis of 23700 tweets is done\n",
      "Sentiment analysis of 23800 tweets is done\n",
      "Sentiment analysis of 23900 tweets is done\n",
      "Sentiment analysis of 24000 tweets is done\n",
      "Sentiment analysis of 24100 tweets is done\n",
      "Sentiment analysis of 24200 tweets is done\n",
      "Sentiment analysis of 24300 tweets is done\n",
      "Sentiment analysis of 24400 tweets is done\n",
      "Sentiment analysis of 24500 tweets is done\n",
      "Sentiment analysis of 24600 tweets is done\n",
      "Sentiment analysis of 24700 tweets is done\n",
      "Sentiment analysis of 24800 tweets is done\n",
      "Sentiment analysis of 24900 tweets is done\n",
      "Sentiment analysis of 25000 tweets is done\n",
      "Sentiment analysis of 25100 tweets is done\n",
      "Sentiment analysis of 25200 tweets is done\n",
      "Sentiment analysis of 25300 tweets is done\n",
      "Sentiment analysis of 25400 tweets is done\n",
      "Sentiment analysis of 25500 tweets is done\n",
      "Sentiment analysis of 25600 tweets is done\n",
      "Sentiment analysis of 25700 tweets is done\n",
      "Sentiment analysis of 25800 tweets is done\n",
      "Sentiment analysis of 25900 tweets is done\n",
      "Sentiment analysis of 26000 tweets is done\n",
      "Sentiment analysis of 26100 tweets is done\n",
      "Sentiment analysis of 26200 tweets is done\n",
      "Sentiment analysis of 26300 tweets is done\n",
      "Sentiment analysis of 26400 tweets is done\n",
      "Sentiment analysis of 26500 tweets is done\n",
      "Sentiment analysis of 26600 tweets is done\n",
      "Sentiment analysis of 26700 tweets is done\n",
      "Sentiment analysis of 26800 tweets is done\n",
      "Sentiment analysis of 26900 tweets is done\n",
      "Sentiment analysis of 27000 tweets is done\n",
      "Sentiment analysis of 27100 tweets is done\n",
      "Sentiment analysis of 27200 tweets is done\n",
      "Sentiment analysis of 27300 tweets is done\n",
      "Sentiment analysis of 27400 tweets is done\n",
      "Sentiment analysis of 27500 tweets is done\n",
      "Sentiment analysis of 27600 tweets is done\n",
      "Sentiment analysis of 27700 tweets is done\n",
      "Sentiment analysis of 27800 tweets is done\n",
      "Sentiment analysis of 27900 tweets is done\n",
      "Sentiment analysis of 28000 tweets is done\n",
      "Sentiment analysis of 28100 tweets is done\n",
      "Sentiment analysis of 28200 tweets is done\n",
      "Sentiment analysis of 28300 tweets is done\n",
      "Sentiment analysis of 28400 tweets is done\n",
      "Sentiment analysis of 28500 tweets is done\n",
      "Sentiment analysis of 28600 tweets is done\n",
      "Sentiment analysis of 28700 tweets is done\n",
      "Sentiment analysis of 28800 tweets is done\n",
      "Sentiment analysis of 28900 tweets is done\n",
      "Sentiment analysis of 29000 tweets is done\n",
      "Sentiment analysis of 29100 tweets is done\n",
      "Sentiment analysis of 29200 tweets is done\n",
      "Sentiment analysis of 29300 tweets is done\n",
      "Sentiment analysis of 29400 tweets is done\n",
      "Sentiment analysis of 29500 tweets is done\n",
      "Sentiment analysis of 29600 tweets is done\n",
      "Sentiment analysis of 29700 tweets is done\n",
      "Sentiment analysis of 29800 tweets is done\n",
      "Sentiment analysis of 29900 tweets is done\n"
     ]
    }
   ],
   "source": [
    "#duygu analizi\n",
    "evlilik['duygu_analizi'] = evlilik.duygu_analizi.astype(str)\n",
    "evlilik[['label', 'score']] = sentiment(evlilik.duygu_analizi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02902cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>para</th>\n",
       "      <th>teklif</th>\n",
       "      <th>yaş</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Üniversitemizin güllerinden Ekrem kardeşimizi ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.943523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huzur, evlilik, para  gordum galiba sugar d4dd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.753243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tribünde önümüzde evlilik teklifi edildi. dald...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.668888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Komucuuu Nasıl önleyebiliriz ? sanırım evlili...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.666369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Winb1r Yüzük nişan evlilik bu kadar denk gelm...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.543826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Evlilik aşkı öldürür dediler daha çok sevdik, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.965834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>@SinemM070327 Evlilik sadece aşkı değil ruhund...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.738472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>Evlilik aşkı değil herşeyi öldürür</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.729743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>Evlilik aşkı öldürür diyorlardı ama ben inanmı...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.697920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>Evlilik aşkı ve seksi öldürür diyo. https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.953501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  para  teklif  yaş  \\\n",
       "0      Üniversitemizin güllerinden Ekrem kardeşimizi ...     0       0    1   \n",
       "1      Huzur, evlilik, para  gordum galiba sugar d4dd...     1       0    0   \n",
       "2      tribünde önümüzde evlilik teklifi edildi. dald...     0       1    0   \n",
       "3      @Komucuuu Nasıl önleyebiliriz ? sanırım evlili...     1       0    0   \n",
       "4      @Winb1r Yüzük nişan evlilik bu kadar denk gelm...     0       1    0   \n",
       "...                                                  ...   ...     ...  ...   \n",
       "29995  Evlilik aşkı öldürür dediler daha çok sevdik, ...     1       0    0   \n",
       "29996  @SinemM070327 Evlilik sadece aşkı değil ruhund...     0       0    0   \n",
       "29997                 Evlilik aşkı değil herşeyi öldürür     0       0    0   \n",
       "29998  Evlilik aşkı öldürür diyorlardı ama ben inanmı...     0       0    0   \n",
       "29999  Evlilik aşkı ve seksi öldürür diyo. https://t....     0       0    0   \n",
       "\n",
       "          label     score  \n",
       "0      negative  0.943523  \n",
       "1      negative  0.753243  \n",
       "2      negative  0.668888  \n",
       "3      negative  0.666369  \n",
       "4      negative  0.543826  \n",
       "...         ...       ...  \n",
       "29995  negative  0.965834  \n",
       "29996  negative  0.738472  \n",
       "29997  positive  0.729743  \n",
       "29998  positive  0.697920  \n",
       "29999  negative  0.953501  \n",
       "\n",
       "[30000 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ödev formatı\n",
    "evlilik[['tweet','para','teklif','yaş','label', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a280524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ödev veri setinin oluşturulması\n",
    "ödev = evlilik[['tweet','para','teklif','yaş','label', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "719b586e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>para</th>\n",
       "      <th>teklif</th>\n",
       "      <th>yaş</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Üniversitemizin güllerinden Ekrem kardeşimizi ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.943523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huzur, evlilik, para  gordum galiba sugar d4dd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.753243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tribünde önümüzde evlilik teklifi edildi. dald...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.668888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Komucuuu Nasıl önleyebiliriz ? sanırım evlili...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.666369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Winb1r Yüzük nişan evlilik bu kadar denk gelm...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.543826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  para  teklif  yaş  \\\n",
       "0  Üniversitemizin güllerinden Ekrem kardeşimizi ...     0       0    1   \n",
       "1  Huzur, evlilik, para  gordum galiba sugar d4dd...     1       0    0   \n",
       "2  tribünde önümüzde evlilik teklifi edildi. dald...     0       1    0   \n",
       "3  @Komucuuu Nasıl önleyebiliriz ? sanırım evlili...     1       0    0   \n",
       "4  @Winb1r Yüzük nişan evlilik bu kadar denk gelm...     0       1    0   \n",
       "\n",
       "      label     score  \n",
       "0  negative  0.943523  \n",
       "1  negative  0.753243  \n",
       "2  negative  0.668888  \n",
       "3  negative  0.666369  \n",
       "4  negative  0.543826  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ödev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9f33709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ödevi kaydetmek\n",
    "ödev.to_csv('ödev-teslim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79428429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
